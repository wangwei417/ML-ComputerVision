{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Image Classification using AutoGluon\n",
    "\n",
    "In this tutorial, we load images and the corresponding labels into [AutoGluon](https://autogluon.mxnet.io/index.html) and use this data to obtain a neural network that can classify new images. This is different from traditional machine learning where we need to manually define the neural network and then specify the hyperparameters in the training process. Instead, with just a single call to AutoGluon’s fit function, AutoGluon automatically trains many models with different hyperparameter configurations and returns the model that achieved the highest level of accuracy.\n",
    "\n",
    "Note: Please use **GPU** for training. CPU training will lead to an unceasing running script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autogluon.mxnet                    0.1.0\r\n",
      "aws-mxnet-cu101mkl                 1.6.0\r\n",
      "keras-mxnet                        2.2.4.2\r\n"
     ]
    }
   ],
   "source": [
    "## make sure that `mxnet-cu101` (for GPU) shows here, rather than `mxnet` (for CPU)\n",
    "!pip list | egrep mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autogluon                          0.1.0\r\n",
      "autogluon-contrib-nlp              0.0.1b20210201\r\n",
      "autogluon.core                     0.1.0\r\n",
      "autogluon.extra                    0.1.0\r\n",
      "autogluon.features                 0.1.0\r\n",
      "autogluon.mxnet                    0.1.0\r\n",
      "autogluon.tabular                  0.1.0\r\n",
      "autogluon.text                     0.1.0\r\n",
      "autogluon.vision                   0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "## install autogluon\n",
    "!pip install -q autogluon\n",
    "!pip list | egrep autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon.core as ag\n",
    "from autogluon.vision import ImagePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use AutoGluon for computer vision task training, we need to organize our data with the following structure:\n",
    "\n",
    "    data/\n",
    "    ├── train/\n",
    "        ├── class1/\n",
    "        ├── class2/\n",
    "        ├── class3/\n",
    "        ├── ...\n",
    "    ├── test/\n",
    "        ├── class1/\n",
    "        ├── class2/\n",
    "        ├── class3/\n",
    "        ├── ...\n",
    "\n",
    "Here each subfolder contains all images that belong to that category, e.g., `class1` contains all images belonging to the first class. We generally recommend at least 100 training images per class for reasonable classification performance, but this might depend on the type of images in your specific use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "For demonstration purposes, we use a subset of the [Shopee-IET](https://www.kaggle.com/c/shopee-iet-machine-learning-competition/data) dataset from Kaggle. Each image in this data depicts a clothing item and the corresponding label specifies its clothing category. Our subset of the data contains the following possible labels: BabyPants, BabyShirt, womencasualshoes, womenchiffontop.\n",
    "\n",
    "We download the data subset and create training/test dataset folders like below. If you use this on your own dataset, just point it to your training or test folder. Example: `train_dataset = ImagePredictor.Dataset.from_folder('mydataset/train')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n",
      "├── test/\n",
      "└── train/\n"
     ]
    }
   ],
   "source": [
    "path = 'https://autogluon.s3.amazonaws.com/datasets/shopee-iet.zip'\n",
    "train_dataset, _, test_dataset = ImagePredictor.Dataset.from_folders(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 image  label\n",
      "0    /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      0\n",
      "1    /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      0\n",
      "2    /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      0\n",
      "3    /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      0\n",
      "4    /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      0\n",
      "..                                                 ...    ...\n",
      "795  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      3\n",
      "796  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      3\n",
      "797  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      3\n",
      "798  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      3\n",
      "799  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...      3\n",
      "\n",
      "[800 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AutoGluon to Fit Models\n",
    "\n",
    "Now, let's fit a __classifier__ using AutoGluon [predictor.fit()](https://auto.gluon.ai/stable/tutorials/image_prediction/beginner.html). Within fit, the dataset is __automatically__ split into training and validation sets. The model with the best hyperparameter configuration is selected based on its performance on the __validation set__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gluoncv.auto.tasks.image_classification:Randomly split train_data into train[736]/validation[64] splits.\n",
      "INFO:gluoncv.auto.tasks.image_classification:Starting fit without HPO\n",
      "INFO:ImageClassificationEstimator:modified configs(<old> != <new>): {\n",
      "INFO:ImageClassificationEstimator:root.valid.batch_size 128 != 16\n",
      "INFO:ImageClassificationEstimator:root.train.rec_train ~/.mxnet/datasets/imagenet/rec/train.rec != auto\n",
      "INFO:ImageClassificationEstimator:root.train.lr        0.1 != 0.01\n",
      "INFO:ImageClassificationEstimator:root.train.rec_val   ~/.mxnet/datasets/imagenet/rec/val.rec != auto\n",
      "INFO:ImageClassificationEstimator:root.train.batch_size 128 != 16\n",
      "INFO:ImageClassificationEstimator:root.train.data_dir  ~/.mxnet/datasets/imagenet != auto\n",
      "INFO:ImageClassificationEstimator:root.train.epochs    10 != 15\n",
      "INFO:ImageClassificationEstimator:root.train.rec_val_idx ~/.mxnet/datasets/imagenet/rec/val.idx != auto\n",
      "INFO:ImageClassificationEstimator:root.train.num_training_samples 1281167 != -1\n",
      "INFO:ImageClassificationEstimator:root.train.rec_train_idx ~/.mxnet/datasets/imagenet/rec/train.idx != auto\n",
      "INFO:ImageClassificationEstimator:root.img_cls.model   resnet50_v1 != resnet50_v1b\n",
      "INFO:ImageClassificationEstimator:}\n",
      "INFO:ImageClassificationEstimator:Saved config to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/config.yaml\n",
      "INFO:ImageClassificationEstimator:Start training from [Epoch 0]\n",
      "INFO:ImageClassificationEstimator:Epoch[0] Batch [49]\tSpeed: 30.431299 samples/sec\taccuracy=0.527500\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 0] training: accuracy=0.527500\n",
      "INFO:ImageClassificationEstimator:[Epoch 0] speed: 29 samples/sec\ttime cost: 32.515403\n",
      "INFO:ImageClassificationEstimator:[Epoch 0] validation: top1=0.776250 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 0] Current best top-1: 0.776250 vs previous 0.000000, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[1] Batch [49]\tSpeed: 46.315775 samples/sec\taccuracy=0.708750\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 1] training: accuracy=0.708750\n",
      "INFO:ImageClassificationEstimator:[Epoch 1] speed: 45 samples/sec\ttime cost: 23.520134\n",
      "INFO:ImageClassificationEstimator:[Epoch 1] validation: top1=0.846250 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 1] Current best top-1: 0.846250 vs previous 0.776250, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[2] Batch [49]\tSpeed: 45.959209 samples/sec\taccuracy=0.721250\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 2] training: accuracy=0.721250\n",
      "INFO:ImageClassificationEstimator:[Epoch 2] speed: 45 samples/sec\ttime cost: 23.659672\n",
      "INFO:ImageClassificationEstimator:[Epoch 2] validation: top1=0.875000 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 2] Current best top-1: 0.875000 vs previous 0.846250, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[3] Batch [49]\tSpeed: 46.266080 samples/sec\taccuracy=0.785000\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 3] training: accuracy=0.785000\n",
      "INFO:ImageClassificationEstimator:[Epoch 3] speed: 45 samples/sec\ttime cost: 23.684071\n",
      "INFO:ImageClassificationEstimator:[Epoch 3] validation: top1=0.908750 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 3] Current best top-1: 0.908750 vs previous 0.875000, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[4] Batch [49]\tSpeed: 45.896277 samples/sec\taccuracy=0.798750\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 4] training: accuracy=0.798750\n",
      "INFO:ImageClassificationEstimator:[Epoch 4] speed: 44 samples/sec\ttime cost: 23.663852\n",
      "INFO:ImageClassificationEstimator:[Epoch 4] validation: top1=0.926250 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 4] Current best top-1: 0.926250 vs previous 0.908750, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[5] Batch [49]\tSpeed: 46.427996 samples/sec\taccuracy=0.800000\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 5] training: accuracy=0.800000\n",
      "INFO:ImageClassificationEstimator:[Epoch 5] speed: 45 samples/sec\ttime cost: 23.448158\n",
      "INFO:ImageClassificationEstimator:[Epoch 5] validation: top1=0.940000 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 5] Current best top-1: 0.940000 vs previous 0.926250, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[6] Batch [49]\tSpeed: 45.969266 samples/sec\taccuracy=0.840000\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 6] training: accuracy=0.840000\n",
      "INFO:ImageClassificationEstimator:[Epoch 6] speed: 45 samples/sec\ttime cost: 23.646251\n",
      "INFO:ImageClassificationEstimator:[Epoch 6] validation: top1=0.952500 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 6] Current best top-1: 0.952500 vs previous 0.940000, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[7] Batch [49]\tSpeed: 46.162280 samples/sec\taccuracy=0.818750\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 7] training: accuracy=0.818750\n",
      "INFO:ImageClassificationEstimator:[Epoch 7] speed: 45 samples/sec\ttime cost: 23.563158\n",
      "INFO:ImageClassificationEstimator:[Epoch 7] validation: top1=0.951250 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:Epoch[8] Batch [49]\tSpeed: 46.326049 samples/sec\taccuracy=0.833750\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 8] training: accuracy=0.833750\n",
      "INFO:ImageClassificationEstimator:[Epoch 8] speed: 45 samples/sec\ttime cost: 23.514306\n",
      "INFO:ImageClassificationEstimator:[Epoch 8] validation: top1=0.962500 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 8] Current best top-1: 0.962500 vs previous 0.952500, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[9] Batch [49]\tSpeed: 45.942313 samples/sec\taccuracy=0.852500\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 9] training: accuracy=0.852500\n",
      "INFO:ImageClassificationEstimator:[Epoch 9] speed: 45 samples/sec\ttime cost: 23.672102\n",
      "INFO:ImageClassificationEstimator:[Epoch 9] validation: top1=0.967500 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 9] Current best top-1: 0.967500 vs previous 0.962500, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[10] Batch [49]\tSpeed: 45.915421 samples/sec\taccuracy=0.855000\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 10] training: accuracy=0.855000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ImageClassificationEstimator:[Epoch 10] speed: 44 samples/sec\ttime cost: 23.684650\n",
      "INFO:ImageClassificationEstimator:[Epoch 10] validation: top1=0.973750 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 10] Current best top-1: 0.973750 vs previous 0.967500, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[11] Batch [49]\tSpeed: 46.075650 samples/sec\taccuracy=0.831250\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 11] training: accuracy=0.831250\n",
      "INFO:ImageClassificationEstimator:[Epoch 11] speed: 45 samples/sec\ttime cost: 23.619562\n",
      "INFO:ImageClassificationEstimator:[Epoch 11] validation: top1=0.976250 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 11] Current best top-1: 0.976250 vs previous 0.973750, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[12] Batch [49]\tSpeed: 46.146087 samples/sec\taccuracy=0.861250\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 12] training: accuracy=0.861250\n",
      "INFO:ImageClassificationEstimator:[Epoch 12] speed: 45 samples/sec\ttime cost: 23.600285\n",
      "INFO:ImageClassificationEstimator:[Epoch 12] validation: top1=0.976250 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:Epoch[13] Batch [49]\tSpeed: 46.228996 samples/sec\taccuracy=0.858750\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 13] training: accuracy=0.858750\n",
      "INFO:ImageClassificationEstimator:[Epoch 13] speed: 45 samples/sec\ttime cost: 23.539262\n",
      "INFO:ImageClassificationEstimator:[Epoch 13] validation: top1=0.982500 top5=1.000000\n",
      "INFO:ImageClassificationEstimator:[Epoch 13] Current best top-1: 0.982500 vs previous 0.976250, saved to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Pickled to /home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0/.trial_0/best_checkpoint.pkl\n",
      "INFO:ImageClassificationEstimator:Epoch[14] Batch [49]\tSpeed: 46.161129 samples/sec\taccuracy=0.875000\tlr=0.010000\n",
      "INFO:ImageClassificationEstimator:[Epoch 14] training: accuracy=0.875000\n",
      "INFO:ImageClassificationEstimator:[Epoch 14] speed: 45 samples/sec\ttime cost: 23.540617\n",
      "INFO:ImageClassificationEstimator:[Epoch 14] validation: top1=0.973750 top5=1.000000\n",
      "INFO:gluoncv.auto.tasks.image_classification:Finished, total runtime is 402.48 s\n",
      "INFO:gluoncv.auto.tasks.image_classification:{ 'best_config': { 'batch_size': 16,\n",
      "                   'dist_ip_addrs': None,\n",
      "                   'epochs': 15,\n",
      "                   'estimator': <class 'gluoncv.auto.estimators.image_classification.image_classification.ImageClassificationEstimator'>,\n",
      "                   'final_fit': False,\n",
      "                   'gpus': [0],\n",
      "                   'log_dir': '/home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0',\n",
      "                   'lr': 0.01,\n",
      "                   'model': 'resnet50_v1b',\n",
      "                   'ngpus_per_trial': 1,\n",
      "                   'nthreads_per_trial': 128,\n",
      "                   'num_trials': 1,\n",
      "                   'num_workers': 4,\n",
      "                   'search_strategy': 'random',\n",
      "                   'seed': 311,\n",
      "                   'time_limits': 600,\n",
      "                   'wall_clock_tick': 1614817101.1394138},\n",
      "  'total_time': 388.5355656147003,\n",
      "  'train_acc': 0.875,\n",
      "  'valid_acc': 0.9825}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.vision.predictor.predictor.ImagePredictor at 0x7fe74f921710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = ImagePredictor()\n",
    "\n",
    "time_limit = 10 * 60 # how long fit() should run (in seconds)\n",
    "predictor.fit(train_dataset,\n",
    "              nets=['ResNet50_v1b', 'ResNet18_v1b'], # default\n",
    "              epochs=10,\n",
    "              time_limit=time_limit,\n",
    "              ngpus_per_trial=1,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results\n",
    "\n",
    "Autogluon also provides the training results, which can be accessed by calling `predictor.fit_summary()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_acc': 0.875,\n",
       " 'valid_acc': 0.9825,\n",
       " 'total_time': 388.5355656147003,\n",
       " 'best_config': {'model': 'resnet50_v1b',\n",
       "  'lr': 0.01,\n",
       "  'num_trials': 1,\n",
       "  'epochs': 15,\n",
       "  'batch_size': 16,\n",
       "  'nthreads_per_trial': 128,\n",
       "  'ngpus_per_trial': 1,\n",
       "  'time_limits': 600,\n",
       "  'search_strategy': 'random',\n",
       "  'dist_ip_addrs': None,\n",
       "  'log_dir': '/home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0',\n",
       "  'num_workers': 4,\n",
       "  'gpus': [0],\n",
       "  'seed': 311,\n",
       "  'final_fit': False,\n",
       "  'estimator': gluoncv.auto.estimators.image_classification.image_classification.ImageClassificationEstimator,\n",
       "  'wall_clock_tick': 1614817101.1394138},\n",
       " 'fit_history': {'train_acc': 0.875,\n",
       "  'valid_acc': 0.9825,\n",
       "  'total_time': 388.5355656147003,\n",
       "  'best_config': {'model': 'resnet50_v1b',\n",
       "   'lr': 0.01,\n",
       "   'num_trials': 1,\n",
       "   'epochs': 15,\n",
       "   'batch_size': 16,\n",
       "   'nthreads_per_trial': 128,\n",
       "   'ngpus_per_trial': 1,\n",
       "   'time_limits': 600,\n",
       "   'search_strategy': 'random',\n",
       "   'dist_ip_addrs': None,\n",
       "   'log_dir': '/home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0',\n",
       "   'num_workers': 4,\n",
       "   'gpus': [0],\n",
       "   'seed': 311,\n",
       "   'final_fit': False,\n",
       "   'estimator': gluoncv.auto.estimators.image_classification.image_classification.ImageClassificationEstimator,\n",
       "   'wall_clock_tick': 1614817101.1394138}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access certain results from this summary. For example, training and validation accuracies below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.875, val acc: 0.983\n"
     ]
    }
   ],
   "source": [
    "print('Train acc: %.3f, val acc: %.3f' %(fit_result['train_acc'], fit_result['valid_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model and optimum hyperparameters: Learning rate, batch size, epochs can be printed with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'resnet50_v1b',\n",
       " 'lr': 0.01,\n",
       " 'num_trials': 1,\n",
       " 'epochs': 15,\n",
       " 'batch_size': 16,\n",
       " 'nthreads_per_trial': 128,\n",
       " 'ngpus_per_trial': 1,\n",
       " 'time_limits': 600,\n",
       " 'search_strategy': 'random',\n",
       " 'dist_ip_addrs': None,\n",
       " 'log_dir': '/home/ec2-user/SageMaker/MLA-CV-Content/notebooks/day_2/392b15c0',\n",
       " 'num_workers': 4,\n",
       " 'gpus': [0],\n",
       " 'seed': 311,\n",
       " 'final_fit': False,\n",
       " 'estimator': gluoncv.auto.estimators.image_classification.image_classification.ImageClassificationEstimator,\n",
       " 'wall_clock_tick': 1614817101.1394138}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_result['fit_history']['best_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the predict function to run on different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BabyShirt</td>\n",
       "      <td>0.522461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class     score  id\n",
       "0  BabyShirt  0.522461   1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = test_dataset.iloc[0]['image']\n",
    "predictor.predict(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               class     score  id  \\\n",
      "0          BabyShirt  0.522461   1   \n",
      "1          BabyShirt  0.957889   1   \n",
      "2   womencasualshoes  0.561329   2   \n",
      "3          BabyPants  0.869628   0   \n",
      "4          BabyPants  0.905809   0   \n",
      "..               ...       ...  ..   \n",
      "75   womenchiffontop  0.975787   3   \n",
      "76   womenchiffontop  0.650768   3   \n",
      "77   womenchiffontop  0.995997   3   \n",
      "78   womenchiffontop  0.983135   3   \n",
      "79   womenchiffontop  0.951276   3   \n",
      "\n",
      "                                                image  \n",
      "0   /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "1   /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "2   /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "3   /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "4   /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "..                                                ...  \n",
      "75  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "76  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "77  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "78  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "79  /home/ec2-user/.gluoncv/datasets/shopee-iet/da...  \n",
      "\n",
      "[80 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "pred = predictor.predict(test_dataset)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
